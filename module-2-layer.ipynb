{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Layer\n",
    "Instead of iterating over all of the inputs we can treat the inputs and the weights as vectors and use a dot product. That is the same as multiplying the elements one by one and summing up the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.808883987593707"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, number_of_inputs):\n",
    "        self.weights = np.random.uniform(-1, 1, size=number_of_inputs)\n",
    "        self.bias = np.random.uniform(-1, 1)\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        return self.weights.dot(inputs) + self.bias\n",
    "    \n",
    "neuron = Neuron(5)\n",
    "\n",
    "neuron.predict(np.array([4, 9, 11, 12, 25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only thing our neuron stores is a weight vector and a bias value. So if we have a list of neuron that all do the same thing we can store all of their weights as a matrix and all of their biases as vector. In other words all the neurons have 1 connection per input and a single output. They don't have connections to each other only to the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22.24100359,  94.87863578, -67.07239799, -28.29942473,\n",
       "        70.58439759])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_inputs = 10\n",
    "num_neurons = 5\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        # we now have 1 row per neuron and 1 column per weight\n",
    "        self.weights = np.random.uniform(-1, 1, size=(num_neurons, num_inputs))\n",
    "\n",
    "        # we also randomly create a bias for each neuron\n",
    "        self.biases = np.random.uniform(-1, 1, size=num_neurons)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.weights.dot(inputs) + self.biases\n",
    "\n",
    "        \n",
    "layer = Layer(num_inputs, num_neurons)\n",
    "\n",
    "# now we have 1 output per neuron in our layer\n",
    "layer.predict(np.array([5, 1, 2, 11, 20, 15, 3, 18, 92, 10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Activation\n",
    "\n",
    "We can also add an activation function to our layer. In That case, the output from each neuron in the layer is passed into the activation function individually. However, there is a case where we use all of the outputs of the layer as the input to the activation function.\n",
    "\n",
    "The softmax activation function takes in all layer outputs at once. Why is it called softmax, well it basically turns all the outputs into a value between 0 and 1. However, unlike the sigmoid activation function, the value of an output is a ratio of the total sum output. In other words we sum up all the outputs and divide each neuron value by the total. This is basically telling us which neuron had the max value (i.e. softmax). For instance if we have 4 output neurons and their output values were 10, 5, 75, 10 the third neuron is obviously the max and their converted softmax values would be .1, .05, .75, .1 . You can see that no neuron can ever be greater than 1 or less than 0. Also, if we're doing a classification the 4 values could represent types of houses ranch, townhouse, duplex, standalone. If a house can only belong to one class the for softmax we would want the value of that class to be 1 and all the rest to be 0. For instance if the house were a townhouse, we would want the layer to output 0, 1, 0, 0. This basically means the network is 100% sure the house is a townhouse.\n",
    "\n",
    "There is one extra step we need for softmax however and that is to use each neuron value as an exponent. The reason for this is to make negative values positive. If the output of softmax is supposed to represent probabilities we can't have them negative. Therefore, instead of summing up all neuron output, we sum up each output as an exponential. We also use the neuron output as an exponential to get the final ratio. That's a lot of jargon so it may be more helpful to look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid layer: [1.0, 0.8375247559137654, 1.0, 1.0]\n",
      "softmax layer: [1.1191390335066746e-13, 2.539919710378433e-34, 0.9999999999998881, 3.992894505348539e-24]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + exp(-z))\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        # we now have 1 row per neuron and 1 column per weight\n",
    "        self.weights = np.random.uniform(-1, 1, size=(num_neurons, num_inputs))\n",
    "\n",
    "        # we also randomly create a bias for each neuron\n",
    "        self.biases = np.random.uniform(-1, 1, size=num_neurons)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        weighted_input = self.weights.dot(inputs) + self.biases\n",
    "        \n",
    "        # now we simply pass each output value into the activation function\n",
    "        return [sigmoid(z) for z in weighted_input]\n",
    "\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        self.weights = np.random.uniform(-1, 1, size=(num_neurons, num_inputs))\n",
    "        self.biases = np.random.uniform(-1, 1, size=num_neurons)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        weighted_input = self.weights.dot(inputs) + self.biases\n",
    "        \n",
    "        # this is where we take each value z and convert it to e^(z)\n",
    "        # so that all values are positive\n",
    "        weighted_input_exp = [exp(z) for z in weighted_input]\n",
    "        \n",
    "        # now we take the total of our exponential values\n",
    "        total = sum(weighted_input_exp)\n",
    "        \n",
    "        # here you can see that when we divide to get the final probability \n",
    "        # we also use the exponential. This makes all our outputs add up to 1\n",
    "        # so that the represent probabilities\n",
    "        return [exp(z)/total for z in weighted_input]\n",
    "    \n",
    "num_inputs, num_neurons = (10, 4)\n",
    "\n",
    "layer = SigmoidLayer(num_inputs, num_neurons)\n",
    "smLayer = SoftmaxLayer(num_inputs, num_neurons)\n",
    "\n",
    "\n",
    "layer_input = np.array([5, 1, 2, 11, 20, 15, 3, 18, 92, 10])\n",
    "\n",
    "# All outputs from the sigmoid layer are between 0 and 1. \n",
    "# However, when we sum them up they may come out to be much greater than 1\n",
    "print('sigmoid layer: {}'.format(layer.predict(layer_input)))\n",
    "\n",
    "# Compare sigmoid to softmax. Here all outputs are guaranteed to sum to 1\n",
    "# This is what we would expect for probabilities of something happening.\n",
    "print('softmax layer: {}'.format(smLayer.predict(layer_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deep\n",
    "\n",
    "We can chain multiple layers together as long as the number of outputs from a previous layer matches the number of inputs to the next. This is what real neural networks look like. Some have dozens or even hundreds of layers.\n",
    "\n",
    "Here we can make the final layer softmax so that our outputs represent a class that the network \"thinks\" the input belongs to. However, we are not limited in the output of our hidden layers. They can have sigmoid, linear, relu, or any other output we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== input \"image\" =====\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === Get the max probability === \n",
      " I think the image is \"number\"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this is a 10X10 matrix of 100 values that represents our \"image\"\n",
    "# which could be a letter, a number, or punctuation\n",
    "image_width = 10\n",
    "num_inputs = image_width*image_width\n",
    "\n",
    "# there are 10 neurons in the first layer\n",
    "num_neurons_l1 = 10\n",
    "\n",
    "# there are 5 neurons in the second layer\n",
    "num_neurons_l2 = 5\n",
    "\n",
    "# there are 3 outputs of our softmax layer\n",
    "# representing 3 image classes. Let's say the \n",
    "# classes are letter, number, punctuation\n",
    "num_outputs = 3\n",
    "\n",
    "layer1 = Layer(num_inputs, num_neurons_l1)\n",
    "layer2 = SigmoidLayer(num_neurons_l1, num_neurons_l2)\n",
    "layer3 = SoftmaxLayer(num_neurons_l2, num_outputs)\n",
    "\n",
    "# lets say these are pixel values that make a \n",
    "# 10X10 = 100 pixel grayscale image. The value\n",
    "# of each pixel can be between 0 and 1\n",
    "network_input = np.random.uniform(0, 1, size=(image_width, image_width))\n",
    "\n",
    "\n",
    "# now we chain the inputs and outputs together to get the final result\n",
    "# for the network input we have to flatten it from a 10X10 matrix to a \n",
    "# vector of length 100\n",
    "output_l1 = layer1.predict(network_input.flatten())\n",
    "\n",
    "output_l2 = layer2.predict(output_l1)\n",
    "\n",
    "final_output = layer3.predict(output_l2)\n",
    "\n",
    "print('===== input \"image\" =====')\n",
    "\n",
    "# show our \"image\"\n",
    "plt.imshow(network_input, shape=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "classes = ['letter', 'number', 'punctuation']\n",
    "# get index of max value\n",
    "\n",
    "class_idx = final_output.index(max(final_output))\n",
    "\n",
    "print('\\n === Get the max probability === \\n I think the image is \"{}\"'.format(classes[class_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We just made a neural network that classifies images!!!!\n",
    "\n",
    "It's not very good because it hasn't learned anything and it's just taking in random noise images. BUT WE HAVE A WORKING NEURAL NETWORK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
