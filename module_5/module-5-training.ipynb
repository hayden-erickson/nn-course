{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Training\n",
    "\n",
    "Our training from before could really use some improvement, but we kept it simple! The first thing we can do is get a better idea of how well our network is doing. \n",
    "\n",
    "Why do we need to change the way we evaluate? Before, when we trained, we included the data that we then used to evaluate. This is basically cheating because it has already learned from the test data so of course it's going to do well on it. Imagine being shown 5 dog pictures, then immediately after inspecting them, you are shown one of those 5 images again and asked to classify it. Of course you would have no trouble.\n",
    "\n",
    "This is where train test comes in. We split the data into a big piece and a small piece. We set aside the small piece and don't use it when training. Therefore, when we want to evaluate the network, we have a true measure of it's performance.\n",
    "\n",
    "For instance, if we're trying to learn about dogs by looking at pictures, then we can learn different features of dogs from the pictures like four legs, furry, big ears, and a tail. Now if you see a new picture of a dog with no tail and very short fur, you might not recognize it. This is the purpose of the test set. There may be features of the dog that we didn't pick up from the original pictures, but are present in the test set. If this is the case then we (or our network) have not learned enough about dogs to make good classifications and we need to keep learning.\n",
    "\n",
    "Let's try withholding some data from the network and use it only to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cost: 179.30\n",
      "prediction\tactual\n",
      "0.1\t\t19.1\n",
      "0.1\t\t20.6\n",
      "0.1\t\t15.2\n",
      "0.1\t\t7.0\n",
      "0.1\t\t8.1\n",
      "0.1\t\t13.6\n",
      "0.1\t\t20.1\n",
      "0.1\t\t21.8\n",
      "0.1\t\t24.5\n",
      "0.1\t\t23.1\n",
      "0.1\t\t19.7\n",
      "0.1\t\t18.3\n",
      "0.1\t\t21.2\n",
      "0.1\t\t17.5\n",
      "0.1\t\t16.8\n",
      "0.1\t\t22.4\n",
      "0.1\t\t20.6\n",
      "0.1\t\t23.9\n",
      "0.1\t\t22.0\n",
      "0.1\t\t11.9\n",
      "average cost: 10.19\n",
      "prediction\tactual\n",
      "18.6\t\t19.1\n",
      "18.6\t\t20.6\n",
      "18.5\t\t15.2\n",
      "18.1\t\t7.0\n",
      "17.9\t\t8.1\n",
      "18.4\t\t13.6\n",
      "18.5\t\t20.1\n",
      "20.0\t\t21.8\n",
      "20.0\t\t24.5\n",
      "20.0\t\t23.1\n",
      "20.0\t\t19.7\n",
      "20.0\t\t18.3\n",
      "20.0\t\t21.2\n",
      "20.0\t\t17.5\n",
      "20.0\t\t16.8\n",
      "20.7\t\t22.4\n",
      "20.7\t\t20.6\n",
      "20.7\t\t23.9\n",
      "20.7\t\t22.0\n",
      "20.7\t\t11.9\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.datasets import load_boston\n",
    "# here I've moved all our functions to a file so we can just worry about training\n",
    "from network import Layer, Network, avg_cost, show_predictions, cost, train\n",
    "\n",
    "dataset = load_boston()\n",
    "house_features = normalize(dataset.data)\n",
    "house_prices = dataset.target\n",
    "                \n",
    "    \n",
    "# the size of our test data\n",
    "test_size = 20\n",
    "\n",
    "# here we'll withold 20 examples to evaluate against\n",
    "train_set = house_features[0:-test_size]\n",
    "train_labels = house_prices[0:-test_size]\n",
    "\n",
    "\n",
    "# this is what we use to evaluate the network\n",
    "test_set = house_features[-test_size:]\n",
    "test_labels = house_prices[-test_size:]\n",
    "\n",
    "num_inputs_l1 = len(house_features[0])\n",
    "num_inputs_l2 = 5\n",
    "\n",
    "l1 = Layer(num_inputs_l1, num_inputs_l2)\n",
    "l2 = Layer(num_inputs_l2, num_inputs_l2)\n",
    "l3 = Layer(num_inputs_l2, 1)\n",
    "\n",
    "net = Network(l1, l2, l3)\n",
    "\n",
    "learning_rate = .00008\n",
    "\n",
    "print('average cost: {:.2f}'.format(avg_cost(test_set, test_labels, net.predict, cost)))\n",
    "\n",
    "print('prediction\\tactual')\n",
    "for (x, y) in zip(test_set, test_labels):\n",
    "    a = net.predict(x)\n",
    "    print('{:.1f}\\t\\t{}'.format(a[0], y))\n",
    "\n",
    "train(net, train_set, train_labels, 10, learning_rate)\n",
    "\n",
    "print('average cost: {:.2f}'.format(avg_cost(test_set, test_labels, net.predict, cost)))\n",
    "\n",
    "print('prediction\\tactual')\n",
    "for (x, y) in zip(test_set, test_labels):\n",
    "    a = net.predict(x)\n",
    "    print('{:.1f}\\t\\t{}'.format(a[0], y))\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "We can take train test a step further and essentially repeat it multiple times. This allows us to get an even more accurate measure of how well our network performs. K folds refers to splitting the data into K, roughly equal sized, pieces and doing train test witholding one of the K folds each time. Therefore, we run train test K times where each time we withold a different piece. \n",
    "\n",
    "After each run where we evaluate the model against a single fold, we discard the model and start over with a new piece. Because we discard the model each time we are basically starting over. The reason for this is to get a more representative idea of how well the model performs. Instead of just 1 score on data we've never seen, we can now have K scores and take the average and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This takes a while. The current progress is shown below\n",
      "batch: 1 2 3 4 5 6 7 8 9 10 \n",
      "[20.5944995  21.57550432 21.8833906  46.30349813 33.28771294 25.38056817\n",
      " 28.23185703 21.79081107 64.60108041 25.47853554]\n",
      "average cost: 30.912745772773917\n",
      "stdev cost: 14.154432536075678\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statistics import mean, stdev\n",
    "from network import avg_cost, cost\n",
    "\n",
    "# this function will chop up our data set for us. \n",
    "# we can specify the number of chunks and whether to shuffle\n",
    "# the data and it will give us back a bunch of indicies\n",
    "# that we can use to grab features and labels from our\n",
    "# entire dataset. This allows us to not have to worry about\n",
    "# how to slice the data set up into roughly equal sized \n",
    "# pieces and withold a different piece each time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# K is the number of groups we will divide the input data into\n",
    "K = 10\n",
    "\n",
    "# the number of times we go through the entire dataset\n",
    "epochs = 50\n",
    "\n",
    "# this is an array where we will save the cost from each of our K runs\n",
    "costs = np.zeros(K)\n",
    "\n",
    "\n",
    "# Here we shuffle the data to prevent the network from learning \n",
    "# any patterns that may be present because of the ordering\n",
    "kfolder = KFold(K, shuffle=True)\n",
    "\n",
    "print('This takes a while. The current progress is shown below')\n",
    "print('batch: ', end='')\n",
    "for ( cost_idx, (train_idxs, test_idxs)) in enumerate(kfolder.split(house_features, house_prices)):\n",
    "    print('{} '.format(cost_idx+1), end='')\n",
    "    train_features, train_labels = house_features[train_idxs], house_prices[train_idxs]\n",
    "    test_features, test_labels = house_features[test_idxs], house_prices[test_idxs]\n",
    "    \n",
    "    # create the new model to train\n",
    "    net = Network(l1, l2, l3)\n",
    "    \n",
    "    train(net, train_features, train_labels, epochs, learning_rate)\n",
    "    \n",
    "    costs[cost_idx] = avg_cost(test_features, test_labels, net.predict, cost)\n",
    "    \n",
    "\n",
    "print()\n",
    "print(costs)\n",
    "print('average cost: {}'.format(mean(costs)))\n",
    "print('stdev cost: {}'.format(stdev(costs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cost can often vary greatly and in some interations it is very low while others it's very high. If we had not used K fold to validate this, we could have got a very low score and assumed that our network was really good when in actuality it isn't. The lower cost could be due to some ordering of the data that causes the network to fit very well, but we can see that on average the cost changes greatly depending on the train set.\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "All examples we've seen so far have used the gradients for every example to update the weights and biases.  The reason we don't want to use every single point to update is that it can be too noisy. There is going to be natural noise in the data and if our network is trying to fit every single bump and curve it can overfit the data and won't generalize well to new examples.\n",
    "\n",
    "The way you can think about noise in the housing data is with similar houses. In a given neighborhood there may be several houses that sell for a similar price. Their features will also be very similar, but not exactly the same. Therefore, even though the house features are slightly different, they have a similar price and we would not want the network to give them all very different predictions just because their features are slightly different. \n",
    "\n",
    "The other extreme is to take the average across all input gradients and use that average to update. There are two drawbacks to this. One is that it makes learning less efficient. In order for our network to learn anything at all, we must first pass in every single house example. The second drawback is that when we average across the entire dataset we loose information. There may be structure in the house examples that is basically smoothed over by the average. There may be outliers that heavily skew the average and make the mean less accurate.\n",
    "\n",
    "For example if we have 5 people whose incomes are 45K, 30K, 53K, 36K, and 200K the average income is 72.8K. That means four people fall below the average income and one falls above. That is not a very good representataion because of the outlier. \n",
    "\n",
    "The same is true for taking the average across the entire house dataset or any dataset for that matter. All of the predicted prices after training become very close to each other. It's as though the network is just predicting the average house value no matter what input it sees.\n",
    "\n",
    "Because such a large average is inefficient and poorly represents the data we can take the average across a smaller batch of examples which is referred to as mini-batch gradient descent. Instead of averaging across all input data, we only take the average of a small portion and use that average to update the weights and biases. Using mini-batch gradient descent, our network can learn faster and pickup on more granular patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cost: 179.59\n",
      "prediction\tactual\n",
      "0.0\t\t19.1\n",
      "0.0\t\t20.6\n",
      "0.0\t\t15.2\n",
      "0.0\t\t7.0\n",
      "0.0\t\t8.1\n",
      "0.0\t\t13.6\n",
      "0.0\t\t20.1\n",
      "0.0\t\t21.8\n",
      "0.0\t\t24.5\n",
      "0.0\t\t23.1\n",
      "0.0\t\t19.7\n",
      "0.0\t\t18.3\n",
      "0.0\t\t21.2\n",
      "0.0\t\t17.5\n",
      "0.0\t\t16.8\n",
      "0.0\t\t22.4\n",
      "0.0\t\t20.6\n",
      "0.0\t\t23.9\n",
      "0.0\t\t22.0\n",
      "0.0\t\t11.9\n",
      "average cost: 8.61\n",
      "prediction\tactual\n",
      "17.6\t\t19.1\n",
      "17.7\t\t20.6\n",
      "17.3\t\t15.2\n",
      "16.7\t\t7.0\n",
      "16.3\t\t8.1\n",
      "17.2\t\t13.6\n",
      "17.4\t\t20.1\n",
      "19.9\t\t21.8\n",
      "19.9\t\t24.5\n",
      "19.9\t\t23.1\n",
      "19.6\t\t19.7\n",
      "19.8\t\t18.3\n",
      "19.8\t\t21.2\n",
      "19.7\t\t17.5\n",
      "19.7\t\t16.8\n",
      "21.1\t\t22.4\n",
      "21.2\t\t20.6\n",
      "21.1\t\t23.9\n",
      "21.1\t\t22.0\n",
      "21.2\t\t11.9\n"
     ]
    }
   ],
   "source": [
    "from network import Network, Layer, avg_cost\n",
    "\n",
    "def avg_batch_gradients(net, features, labels):\n",
    "    avg_dC_dws = [np.zeros(shape=l.weights.shape) for l in net.layers]\n",
    "    avg_dC_dbs = [np.zeros(shape=l.biases.shape) for l in net.layers]\n",
    "    \n",
    "    for (x, y) in zip(features, labels):\n",
    "        dC_dw, dC_db = net.get_gradient(x, y)\n",
    "        \n",
    "        for j in range(len(net.layers)):\n",
    "            avg_dC_dws[j] += dC_dw[j]/len(features)\n",
    "            avg_dC_dbs[j] += dC_db[j]/len(features)\n",
    "        \n",
    "    return avg_dC_dws, avg_dC_dbs\n",
    "\n",
    "def batch_train(net, features, labels, epochs, learning_rate, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        num_batches = int(len(features)/batch_size)\n",
    "\n",
    "        # We iterate over each batch instead of every single training example\n",
    "        for i in range(num_batches):\n",
    "            batch_range = range(i*batch_size, (i+1)*batch_size)\n",
    "    \n",
    "            dC_dws, dC_dbs = avg_batch_gradients(\n",
    "                 net, features[batch_range], labels[batch_range])\n",
    "\n",
    "            # LEARNING!!!\n",
    "            # update weights and biases\n",
    "            for j in range(len(net.layers)):\n",
    "                net.layers[j].weights -= dC_dws[j] * learning_rate\n",
    "                net.layers[j].biases -= dC_dbs[j].flatten() * learning_rate\n",
    "\n",
    "                \n",
    "# the size of our test data\n",
    "test_size = 20\n",
    "batch_size = 10\n",
    "learning_rate = .008\n",
    "epochs = 50\n",
    "\n",
    "# here we'll withold 20 examples to evaluate against\n",
    "train_set, train_labels = house_features[0:-test_size], house_prices[0:-test_size]\n",
    "test_features, test_labels = house_features[-test_size:], house_prices[-test_size:]\n",
    "\n",
    "net = Network(\n",
    "    Layer(13, 8),\n",
    "    Layer(8, 1))\n",
    "\n",
    "\n",
    "print('average cost: {:.2f}'.format(\n",
    "    avg_cost(test_features, test_labels, net.predict, cost)))\n",
    "\n",
    "print('prediction\\tactual')\n",
    "for (x, y) in zip(test_set, test_labels):\n",
    "    a = net.predict(x)\n",
    "    print('{:.1f}\\t\\t{}'.format(a[0], y))\n",
    "\n",
    "batch_train(net, train_set, train_labels, epochs, learning_rate, batch_size)\n",
    "\n",
    "print('average cost: {:.2f}'.format(\n",
    "    avg_cost(test_features, test_labels, net.predict, cost)))\n",
    "\n",
    "print('prediction\\tactual')\n",
    "for (x, y) in zip(test_set, test_labels):\n",
    "    a = net.predict(x)\n",
    "    print('{:.1f}\\t\\t{}'.format(a[0], y))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Activation Functions\n",
    "\n",
    "Up until now we have used the linear activation function $y = x$. This is fine if our task is regression. In other words if our network is trying to predict continuous values such as house prices or coordinates. However, if we want our network to perform classification, we don't want it to give us a number we want it to give us a class. Specifically, we want the network to tell us what class the input belongs to and the probability.\n",
    "\n",
    "For instance if we have a network that can distinguish between dogs, cats, and pigs and we give it an image of a dog, we want the network to say that it is confident the image is a dog by means of a percentage value. For example it might give the dog class output a score of 98% and the other two classes a score of 1%. Interpreting this percentage we might say the network is 98% confident the image is a dog.\n",
    "\n",
    "To do this type of classification we would need three output neurons from the network and we would have to use an activation function that will only give a value between 0 and 1. For our case we can use the sigmoid function.\n",
    "\n",
    "Because we have a dataset from scikit learn that includes images of handwritten digits, we will use that for our classification network. The handwritten digits dataset has ten different classes as you can imagine, one for each digit 0-9. Therefore, we need a network with 10 output neurons.\n",
    "\n",
    "As for the inputs of the network, we are using images, but how do we feed an image into the network? we flatten it into a 1 dimensional array and that array becomes a single row in the dataset. The images have 64 pixels meaning the image when viewed is 8X8 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.308739165648047\n",
      "actual\t\tprediction\tcorrect\n",
      "------\t\t----------\t-------\n",
      "4\t\t0\t\t\n",
      "6\t\t1\t\t\n",
      "6\t\t4\t\t\n",
      "6\t\t0\t\t\n",
      "4\t\t4\t\t✓\n",
      "9\t\t4\t\t\n",
      "1\t\t4\t\t\n",
      "5\t\t1\t\t\n",
      "0\t\t1\t\t\n",
      "9\t\t4\t\t\n",
      "5\t\t0\t\t\n",
      "2\t\t1\t\t\n",
      "8\t\t4\t\t\n",
      "0\t\t1\t\t\n",
      "1\t\t4\t\t\n",
      "7\t\t4\t\t\n",
      "6\t\t7\t\t\n",
      "3\t\t4\t\t\n",
      "2\t\t4\t\t\n",
      "1\t\t4\t\t\n",
      "=== TRAINING ===\n",
      "cost: 0.04768696012438989\n",
      "actual\t\tprediction\tcorrect\n",
      "------\t\t----------\t-------\n",
      "4\t\t4\t\t✓\n",
      "6\t\t6\t\t✓\n",
      "6\t\t6\t\t✓\n",
      "6\t\t6\t\t✓\n",
      "4\t\t1\t\t\n",
      "9\t\t9\t\t✓\n",
      "1\t\t1\t\t✓\n",
      "5\t\t5\t\t✓\n",
      "0\t\t0\t\t✓\n",
      "9\t\t9\t\t✓\n",
      "5\t\t5\t\t✓\n",
      "2\t\t2\t\t✓\n",
      "8\t\t8\t\t✓\n",
      "0\t\t0\t\t✓\n",
      "1\t\t1\t\t✓\n",
      "7\t\t7\t\t✓\n",
      "6\t\t6\t\t✓\n",
      "3\t\t3\t\t✓\n",
      "2\t\t2\t\t✓\n",
      "1\t\t1\t\t✓\n"
     ]
    }
   ],
   "source": [
    "import random as rand\n",
    "from sklearn.datasets import load_digits\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cost(a, y, derivative=False):\n",
    "    if derivative:\n",
    "        return a - y\n",
    "\n",
    "    return .5*norm(y - a)**2\n",
    "\n",
    "# The sigmoid function will only output a number between 0 and 1\n",
    "# which we can use to represent a probability. It also has the\n",
    "# very nice property that it's derivative is just s(z)*(1-s(z))\n",
    "# where s is the sigmoid function.\n",
    "\n",
    "\n",
    "def sigmoid(z, derivative=False):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "\n",
    "    if not derivative:\n",
    "        return s\n",
    "\n",
    "    return s*(1-s)\n",
    "\n",
    "\n",
    "def linear(z, derivative=False):\n",
    "    if not derivative:\n",
    "        return z\n",
    "\n",
    "    return np.ones(shape=z.shape)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    # now we will allow the creation of a layer with the activation function as a parameter\n",
    "    # that we we can use softmax or any other function instead of just linear\n",
    "    def __init__(self, num_inputs, num_neurons, activation_fn=linear):\n",
    "        self.weights = np.random.uniform(-.1, .1,\n",
    "                                         size=(num_neurons, num_inputs))\n",
    "        self.biases = np.random.uniform(-.1, .1, size=(num_neurons))\n",
    "        self.activation = activation_fn\n",
    "        self.dz_dw = []\n",
    "        self.da_dz = []\n",
    "\n",
    "    def predict(self, features):\n",
    "        z = self.weights.dot(features) + self.biases\n",
    "        # here we save the activation function gradient to use later\n",
    "        # when we perform gradient descent for the whole network\n",
    "        self.da_dz = self.activation(z, derivative=True)\n",
    "        self.dz_dw = self.activation(z)\n",
    "        return self.dz_dw\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.grad_calls = 0\n",
    "        self.dz_dw = []\n",
    "        self.da_dz = []\n",
    "\n",
    "    def get_gradient(self, x, y):\n",
    "        # find the gradient for a single example\n",
    "        prediction = self.predict(x)\n",
    "        dC_da = cost(prediction, y, derivative=True)\n",
    "\n",
    "        dC_dw = []\n",
    "        dC_db = []\n",
    "\n",
    "        for i in range(1, len(self.layers)+1):\n",
    "            # the line below is the only difference from our previous\n",
    "            # network. Instead of assuming that da/dz = 1 (which it\n",
    "            # does for our linear activation layers) we must use the\n",
    "            # gradient for any activation that the layer has. In this\n",
    "            # example it is sigmoid for the last layer.\n",
    "            dC_dz = dC_da * self.da_dz[-i]\n",
    "            dC_dw.insert(0, np.outer(dC_dz, self.dz_dw[-(i+1)]))\n",
    "            dC_db.insert(0, dC_dz)\n",
    "            dC_da = self.layers[-i].weights.transpose().dot(dC_dz)\n",
    "\n",
    "        self.grad_calls += 1\n",
    "        return dC_dw, dC_db\n",
    "\n",
    "    def predict(self, x):\n",
    "        da_dz = []\n",
    "        dz_dw = [x]\n",
    "        prediction = x\n",
    "\n",
    "        for l in self.layers:\n",
    "            prediction = l.predict(prediction)\n",
    "            da_dz.append(l.da_dz)\n",
    "            dz_dw.append(prediction)\n",
    "\n",
    "        self.da_dz = da_dz\n",
    "        self.dz_dw = dz_dw\n",
    "\n",
    "        return prediction\n",
    "\n",
    "# label_to_vector takes in a digit lable 0-9\n",
    "# and turns it into a vector of probabilities\n",
    "# where all values are 0 except for the label\n",
    "# index which is 1. For example, if the label\n",
    "# were 3, our output vector would have 1 at\n",
    "# index 3 and 0 everywhere else. i.e.\n",
    "# [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "def label_to_vector(digit):\n",
    "    probability_vector = np.zeros(10)\n",
    "    probability_vector[digit] = 1\n",
    "    return probability_vector\n",
    "\n",
    "# vector_to_label takes in a vector of probabilities\n",
    "# where each element corresponds to the probability\n",
    "# of a digit. For instance, the first value is the\n",
    "# probability of the input being 0, the second value\n",
    "# being 1, third being 2, and so on. It finds the\n",
    "# index with the max probability and returns it.\n",
    "# Therefore, if our network predicted index 8\n",
    "# to have the highest value, we would say it has\n",
    "# predicted the input digit image to be 8. i.e.\n",
    "# [.01, .02, .31, .22, .14, .06, .02, .1, .93, .4]\n",
    "\n",
    "\n",
    "def vector_to_label(vec):\n",
    "    return np.argmax(vec)\n",
    "\n",
    "\n",
    "def show_predictions(digit_images, digit_labels):\n",
    "    print('actual\\t\\tprediction\\tcorrect')\n",
    "    print('------\\t\\t----------\\t-------')\n",
    "    for (x, y) in zip(digit_images, digit_labels):\n",
    "        actual = y\n",
    "        prediction = vector_to_label(net.predict(x))\n",
    "        print('{}\\t\\t{}\\t\\t{}'.format(actual, prediction,\n",
    "                                      '✓' if actual == prediction else ''))\n",
    "\n",
    "\n",
    "dataset = load_digits()\n",
    "\n",
    "digit_images = dataset.data\n",
    "digit_labels = dataset.target\n",
    "\n",
    "test_start = rand.randint(0, len(digit_images)-20)\n",
    "test_set = digit_images[test_start:test_start+20]\n",
    "test_labels = digit_labels[test_start:test_start+20]\n",
    "\n",
    "\n",
    "l1 = Layer(64, 32)\n",
    "l2 = Layer(32, 16)\n",
    "l3 = Layer(16, 10, activation_fn=sigmoid)\n",
    "net = Network(l1, l2, l3)\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 5\n",
    "learning_rate = .008\n",
    "\n",
    "# the digit labels are the actual digit that the images corresponds to 0-9\n",
    "# we have to convert this to a vector of probabilities that our network outputs\n",
    "labels = np.array([label_to_vector(label) for label in digit_labels])\n",
    "test_label_vectors = [label_to_vector(label) for label in test_labels]\n",
    "\n",
    "print('cost: {}'.format(avg_cost(test_set, test_label_vectors, net.predict, cost)))\n",
    "show_predictions(test_set, test_labels)\n",
    "\n",
    "print('=== TRAINING ===')\n",
    "batch_train(net, digit_images, labels, epochs, learning_rate, batch_size)\n",
    "\n",
    "print('cost: {}'.format(avg_cost(test_set, test_label_vectors, net.predict, cost)))\n",
    "show_predictions(test_set, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
